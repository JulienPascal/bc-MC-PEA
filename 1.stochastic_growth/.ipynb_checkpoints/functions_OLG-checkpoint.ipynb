{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbae77f",
   "metadata": {},
   "source": [
    "# Functions for OLG\n",
    "\n",
    "Create functions used when solving an OLG Model with the [bc-MC Operator](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4476122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034146bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import quantecon as qe\n",
    "from interpolation import interp\n",
    "from quantecon.optimize import brentq\n",
    "from numba import njit, float64\n",
    "from numba.experimental import jitclass\n",
    "#import Tasmanian # sparse grids\n",
    "\n",
    "import random\n",
    "import scipy.stats\n",
    "import chaospy  ## for quadrature\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "import time\n",
    "from math import sqrt\n",
    "import seaborn as sns; sns.set()\n",
    "from tqdm import tqdm as tqdm         # tqdm is a nice library to visualize ongoing loops\n",
    "import datetime\n",
    "# followint lines are used for indicative typing\n",
    "from typing import Tuple\n",
    "class Vector: pass\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "# To create copies of NN\n",
    "import copy\n",
    "import matplotlib.ticker as mtick\n",
    "# To use sparse kronecker product\n",
    "from scipy import sparse\n",
    "from torchcontrib.optim import SWA\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a1bca-66b8-4955-b7b2-df3916430c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pea_init(param):\n",
    "    \"\"\"\n",
    "    Solves the log-linear version of the stochastic growth model.\n",
    "    Then fit a polynomial. Provides a good starting point for PEA.\n",
    "    \n",
    "    Parameters:\n",
    "      e     : 1D numpy array of shocks, with length at least long+init.\n",
    "      param : params classs\n",
    "    \n",
    "    Returns:\n",
    "      theta: Estimated parameter vector from the regression.\n",
    "    \"\"\"\n",
    "    # Unpack structural parameters\n",
    "    ab = 0\n",
    "    long = 100000\n",
    "    init = 500\n",
    "    slong = init + long\n",
    "    \n",
    "    alpha = params.alpha \n",
    "    beta = params.beta\n",
    "    delta = params.delta\n",
    "    rho = params.rho_tfp\n",
    "    se = params.std_tfp\n",
    "    sigma = params.gamma\n",
    "    \n",
    "    long = int(long)\n",
    "    init = int(init)\n",
    "    \n",
    "    # Dimensions of the system\n",
    "    ncont = 3          # Number of static equations\n",
    "    nbend = 1          # Number of endogenous predetermined variables\n",
    "    nshoc = 1          # Number of shocks\n",
    "    nback = nbend + nshoc  # Number of state variables\n",
    "    nforw = 1          # Number of costate variables\n",
    "    nstat = nback + nforw  # Total state and costate variables\n",
    "\n",
    "    # Shocks\n",
    "    e = params.std_tfp * np.random.randn(slong)\n",
    "    \n",
    "    # Preallocate coefficient matrices\n",
    "    Mcc  = np.zeros((ncont, ncont))\n",
    "    Mcs  = np.zeros((ncont, nstat))\n",
    "    Mss0 = np.zeros((nstat, nstat))\n",
    "    Mss1 = np.zeros((nstat, nstat))\n",
    "    Msc0 = np.zeros((nstat, ncont))\n",
    "    Msc1 = np.zeros((nstat, ncont))\n",
    "    Mse  = np.zeros((nstat, nshoc))\n",
    "    \n",
    "    # Compute steady state values\n",
    "    ysk = (1 - beta*(1-delta)) / (alpha * beta)\n",
    "    ksy = 1 / ysk\n",
    "    ys  = ksy ** (alpha / (1 - alpha))\n",
    "    ks  = ys ** (1 / alpha)\n",
    "    is_ = delta * ks   # Investment\n",
    "    cs  = ys - is_\n",
    "    ls  = cs ** (-sigma)\n",
    "    \n",
    "    # Construct the static equations matrices\n",
    "    # 1) Output equation (k, z, c)\n",
    "    Mcc[0, 0] = 1\n",
    "    Mcs[0, 0] = alpha\n",
    "    Mcs[0, 1] = 1\n",
    "\n",
    "    # 2) Investment equation\n",
    "    Mcc[1, 0] = 1\n",
    "    Mcc[1, 1] = -is_ / ys\n",
    "    Mcc[1, 2] = -cs / ys\n",
    "\n",
    "    # 3) Consumption equation\n",
    "    Mcc[2, 2] = -sigma\n",
    "    Mcs[2, 2] = 1\n",
    "\n",
    "    # Capital (state) equation\n",
    "    Mss0[0, 0] = 1\n",
    "    Mss1[0, 0] = delta - 1\n",
    "    Msc1[0, 1] = delta\n",
    "\n",
    "    # Technology shock equation\n",
    "    Mss0[1, 1] = 1\n",
    "    Mss1[1, 1] = -rho\n",
    "    Mse[1, 0] = 1\n",
    "\n",
    "    # Euler equation\n",
    "    Mss0[2, 0] = (1 - beta*(1-delta))\n",
    "    Mss0[2, 2] = -1\n",
    "    Mss1[2, 2] = 1\n",
    "    Msc0[2, 0] = (1 - beta*(1-delta))\n",
    "\n",
    "    # Solve the system of equations\n",
    "    M0 = np.linalg.inv(Mss0 - Msc0 @ np.linalg.inv(Mcc) @ Mcs)\n",
    "    M1 = Mss1 - Msc1 @ np.linalg.inv(Mcc) @ Mcs\n",
    "    W  = - M0 @ M1\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors of W\n",
    "    MU, P = np.linalg.eig(W)\n",
    "    # Sort eigenvalues by their absolute values\n",
    "    idx = np.argsort(np.abs(MU))\n",
    "    MU = MU[idx]\n",
    "    P = P[:, idx]\n",
    "    Q = np.linalg.inv(P)\n",
    "\n",
    "    # Compute Gamma from Q:\n",
    "    # In Matlab: Gamma = -inv(Q(nback+1:nstat, nback+1:nstat)) * Q(nback+1:nstat, 1:nback)\n",
    "    # In Python, note that indices start at 0.\n",
    "    Gamma = - np.linalg.inv(Q[nback:nstat, nback:nstat]) @ Q[nback:nstat, :nback]\n",
    "\n",
    "    # Compute the transition matrices MSS and PI\n",
    "    MSS = W[:nback, :nback] + W[:nback, nback:nstat] @ Gamma\n",
    "    PI  = np.linalg.inv(Mcc) @ (Mcs[:, :nback] + Mcs[:, nback:nstat] @ Gamma)\n",
    "    MSE = np.vstack([np.zeros((nbend, nshoc)), np.eye(nshoc)])\n",
    "\n",
    "    # Simulation: generate state vector S over time with dimensions (nback, long+init)\n",
    "    S = np.zeros((nback, long + init))\n",
    "    S[:, 0] = MSE @ e[0:1]  # Use the first shock\n",
    "\n",
    "    for i in range(1, long + init):\n",
    "        S[:, i] = MSS @ S[:, i-1] + MSE @ e[i:i+1]\n",
    "\n",
    "    # Compute policy functions from simulated states\n",
    "    lb = Gamma @ S\n",
    "    lb = ls * np.exp(lb)\n",
    "    lb = lb.flatten()\n",
    "\n",
    "    k = np.log(ks) + S[0, :]\n",
    "    k = k.flatten()\n",
    "    ek = np.exp(k)\n",
    "    \n",
    "    a = S[1, :].flatten()  # Technology shock state\n",
    "    ea = np.exp(a)\n",
    "    \n",
    "    # Define time indices for regression\n",
    "    # In Matlab: T = init+1 : init+long-1, T1 = init+2 : init+long\n",
    "    # Adjust for zero-indexing in Python:\n",
    "    T  = np.arange(init, init + long - 1)\n",
    "    T1 = np.arange(init + 1, init + long)\n",
    "\n",
    "    # Construct regression matrix X and outcome y\n",
    "    # X columns: [1, k, a, k^2, a^2, k*a]\n",
    "    X = np.column_stack([\n",
    "        np.ones(len(T)),\n",
    "        k[T],\n",
    "        a[T],\n",
    "        k[T]**2,\n",
    "        a[T]**2,\n",
    "        k[T] * a[T]\n",
    "    ])\n",
    "    \n",
    "    y = np.log(beta * lb[T1] * (alpha * ea[T1] * ek[T1]**(alpha-1) + 1 - delta))\n",
    "    \n",
    "    # Solve for theta using least squares regression\n",
    "    theta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "    \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cba15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "# Ouputs:\n",
    "# 1. the share of cash-in-hand consumed\n",
    "# 2. the lagrange multiplier h \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba1e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_params(params, limited=True):\n",
    "    \"\"\"\n",
    "    Function to display parameter values\n",
    "    \"\"\"\n",
    "    print(\"learning rate: {}\".format(params.lr))\n",
    "    print(\"nb epochs: {}\".format(params.nb_epochs))\n",
    "    print(\"W_expanded.shape: {}\".format(params.W_expanded.shape))\n",
    "    print(\"M: {}\".format(params.M))\n",
    "    print(\"N: {}\".format(params.N))\n",
    "    print(\"MN: {}\".format(params.MN))\n",
    "    print(\"T: {}\".format(params.T))\n",
    "    print(\"optimizer_chosen: {}\".format(params.optimizer))\n",
    "    print(\"use_scheduler: {}\".format(params.use_scheduler))\n",
    "    print(\"nb agents: {}\".format(params.nb_agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape) \n",
    "\n",
    "# RMSE\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "# Gaussian quadrature rule\n",
    "# See: https://chaospy.readthedocs.io/en/master/api/chaospy.generate_quadrature.html\n",
    "def dist(order, distribution, rule = \"gaussian\", sp=True):\n",
    "    #order=int(n**(1/d))-1\n",
    "    x, w = chaospy.generate_quadrature(order, distribution, rule=(rule), sparse=sp)\n",
    "    return x, w\n",
    "\n",
    "def create_W_expanded_matrix(M, N, rep):\n",
    "    \"\"\"\n",
    "    create a sparse matrix W_expanded with U repeate M times on the diagonal elements\n",
    "    where U is an upper triangular matrix with 0 on the diagonal and 1 on the other upper elements\n",
    "    W_expanded is a sparse torch matrix\n",
    "    rep: number of times the matrix must be repeat vertically\n",
    "    \"\"\"\n",
    "    A_expanded = np.ones((N, N))\n",
    "    U = np.triu(A_expanded) # upper trianguler matrix of ones\n",
    "    np.fill_diagonal(U, 0) #fill diagonal with 0\n",
    "    U = sparse.csr_matrix(U) # convert to sparse\n",
    "    # Unity matrix of size (M*M)\n",
    "    B = sparse.csr_matrix(np.eye(M, M))\n",
    "    D = sparse.kron(B, U)\n",
    "    # To \"repeat\" D vertically M times\n",
    "    #D_repeated_vertical = sparse.vstack([D] * rep )\n",
    "    #D_repeated_horizontal = sparse.hstack([D] * rep)\n",
    "    # create a larger block diagonal matrix with D on the diagonal\n",
    "    I_rep = sparse.csr_matrix(np.eye(rep, rep))\n",
    "    D_repeated = sparse.kron(I_rep, D)\n",
    "    \n",
    "    # Convert to sparse tensor\n",
    "    W_expanded = sparse_mx_to_torch_sparse_tensor(D_repeated)\n",
    "    \n",
    "    return W_expanded\n",
    "\n",
    "min_FB = lambda a,b: a+b-tf.sqrt(a**2+b**2)\n",
    "min_FB_torch = lambda a,b: a+b-torch.sqrt(a**2+b**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcca2a",
   "metadata": {},
   "source": [
    "## Data management with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d359a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_states(dataloader_replacement):\n",
    "    \"\"\"\n",
    "    Simulate state vector. Use data to approximate density.\n",
    "    Input is a pytorch data loader (with replacement)\n",
    "    \"\"\"\n",
    "    # Create an iterator from the DataLoader\n",
    "    iterator = iter(dataloader_replacement)\n",
    "    # Draw one batch using next\n",
    "    batch = next(iterator)  \n",
    "    return batch\n",
    "\n",
    "def simulate_innovations(params, len_series):\n",
    "    # randomly drawing innovations\n",
    "    if params.distribution_shocks == \"Normal\":\n",
    "        e_tfp = torch.normal(mean=0.0, std=params.std_tfp, size=(len_series,)) \n",
    "        e_delta = torch.normal(mean=0.0, std=params.std_delta, size=(len_series,)) \n",
    "    else:\n",
    "        raise Exception(f\"{params.distribution_shocks} not implemented.\") \n",
    "    return e_tfp, e_delta \n",
    "\n",
    "def simulate_shocks_given_innovations(params, len_series, e_tfp, e_delta):\n",
    "    # Simulate series\n",
    "    if params.distribution_shocks == \"Normal\":\n",
    "        # no persistence\n",
    "        series_delta = params.mean_delta + e_delta\n",
    "        # Persistence\n",
    "        log_series_tfp = torch.zeros_like(e_tfp)\n",
    "        for i in range(1, len(e_tfp)):\n",
    "            log_series_tfp[i] = params.rho_tfp*log_series_tfp[i-1] + e_tfp[i]\n",
    "        # In level\n",
    "        series_tfp = torch.exp(log_series_tfp)\n",
    "        # Ensure we don't get negative values\n",
    "        if (torch.sum((series_delta <0.0)) > 0):\n",
    "            raise Exception(f\"Negative values happened for delta\") \n",
    "    else:\n",
    "        raise Exception(f\"{params.distribution_shocks} not implemented.\") \n",
    "    return series_tfp, series_delta \n",
    "    \n",
    "def simulate_shocks(params, len_series):\n",
    "    # Draw innovations\n",
    "    e_tfp, e_delta = simulate_innovations(params, len_series)\n",
    "    series_tfp, series_delta = simulate_shocks_given_innovations(params, len_series, e_tfp, e_delta)\n",
    "    return series_tfp, series_delta \n",
    "    \n",
    "def simulate_innovations_and_shocks(params, len_series):\n",
    "    # Return innovations and shocks\n",
    "    # Draw innovations\n",
    "    e_tfp, e_delta = simulate_innovations(params, len_series)\n",
    "    series_tfp, series_delta = simulate_shocks_given_innovations(params, len_series, e_tfp, e_delta)\n",
    "    return e_tfp, e_delta, series_tfp, series_delta \n",
    "\n",
    "class InfiniteSampler(torch.utils.data.Sampler):\n",
    "    \"\"\"Infinite Sampler that generates infinite indices by sampling with replacement.\"\"\"\n",
    "    def __init__(self, data_source):\n",
    "        self.num_samples = len(data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:  # Infinite loop\n",
    "            yield torch.randint(high=self.num_samples, size=(1,), dtype=torch.int64).item()\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class.\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def generate_data_debug(params, T, N):\n",
    "    \"\"\"\n",
    "    Generate test data. Random draws from Uniform.\n",
    "    \"\"\"\n",
    "    # T: Number of observations. represents length of simulation\n",
    "    # N: Dimension of each observation vector. dimension of state vector.\n",
    "    # Generate random observations (T vectors, each of N dimensions)\n",
    "    # observations = torch.rand(T, N) + 1e-6 #torch.randn(T, N)\n",
    "    \n",
    "    # random capital allocation vectors\n",
    "    # row: obs\n",
    "    # col: variable\n",
    "    r1 = 0.25\n",
    "    r2 = 3.0\n",
    "    distribution_capital = (r1 - r2) * torch.rand(T, params.nb_agents) + r2\n",
    "    distribution_capital[:,0] = 0\n",
    "\n",
    "    # random exo state vetors:\n",
    "    e_tfp, e_delta = simulate_shocks(params, T)\n",
    "    exo_states = torch.column_stack((e_tfp, e_delta))\n",
    "    \n",
    "    observations = torch.hstack((distribution_capital, exo_states)) \n",
    "    return observations\n",
    "\n",
    "def generate_dataloaders(observations, batch_size):\n",
    "    \"\"\"\n",
    "    Generate dataloaders\n",
    "    \"\"\"\n",
    "    # batch_size: dimension of each draw. (M)\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(observations)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    ## Draw without replacement\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    ## Draw with replacement\n",
    "    # Create a DataLoader with the InfiniteSampler\n",
    "    dataset = MyDataset(observations)\n",
    "    dataloader_with_replacement = DataLoader(dataset, batch_size=batch_size, sampler=InfiniteSampler(dataset))\n",
    "\n",
    "    return dataloader, dataloader_with_replacement\n",
    "\n",
    "def generate_data_and_dataloaders_debug(params, T, N, batch_size):\n",
    "    if T < batch_size:\n",
    "        raise Exception(f\"T: {T} < batch_size: {batch_size}\") \n",
    "    observations = generate_data_debug(params, T, N)\n",
    "    dataloader, dataloader_with_replacement = generate_dataloaders(observations, batch_size)\n",
    "    return dataloader, dataloader_with_replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8543a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_batches(nb_batches, d_replacement):\n",
    "    \"\"\"\n",
    "    Draw nb_draws times M draws from ergodic distribution\n",
    "    \"\"\"\n",
    "    for i in range(0, nb_batches):\n",
    "        if i == 0:\n",
    "            state_vec = sim_states(d_replacement)\n",
    "        else:\n",
    "            state_vec = torch.vstack((state_vec, sim_states(d_replacement)))\n",
    "    return state_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fbe8b",
   "metadata": {},
   "source": [
    "Generate data using neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c6b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_current_model(neural_net, initial_state_vector, len_T, params, use_true_model = False, non_stochastic_ss = False, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Use current neural net to simulate the model\n",
    "    Input:\n",
    "    neural_net: a pytorch neural network\n",
    "    len_T: length of simulation\n",
    "    params: a Params object\n",
    "    use_true_model: if true, use analytic solution. Else, use neural net\n",
    "    non_stochastic_ss: if true, simulation with exo variables constant\n",
    "    \"\"\"\n",
    "    #add burnin\n",
    "    burnin = int(len_T/10) #10% burnin\n",
    "    #print(burnin)\n",
    "    T = len_T + burnin\n",
    "    \n",
    "    # random draws\n",
    "    if non_stochastic_ss == False:\n",
    "        tfp_vec, delta_vec = simulate_shocks(params, T)\n",
    "    else:\n",
    "        tfp_vec = params.mean_tfp*torch.ones(T)\n",
    "        delta_vec = params.mean_delta*torch.ones(T)\n",
    "\n",
    "    #distribution capital holdings:\n",
    "    h_matrix = torch.zeros((T, params.nb_agents))\n",
    "    h_matrix[0,1:] = assets_SS #initial_state_vector #first periods\n",
    "    K_vec = torch.zeros(T)\n",
    "    \n",
    "    ## Exogeneous labour supply. Work until period A_tilde\n",
    "    l_matrix = torch.zeros((T, params.nb_agents))\n",
    "    l_matrix[:, 0:params.A_tilde+1] = params.l_s #supply ls units of labor from 0 until A_tilde (included).\n",
    "    \n",
    "    L_vec = torch.zeros(T)\n",
    "    r = torch.zeros(T)\n",
    "    w = torch.zeros(T)\n",
    "    wealth = torch.zeros((T, params.nb_agents))\n",
    "    a_matrix = torch.zeros((T, params.nb_agents))\n",
    "    a_matrix[0,1:] = assets_SS #initial_state_vector\n",
    "    c_matrix = torch.zeros((T, params.nb_agents))\n",
    "\n",
    "    # Loop over time periods:\n",
    "    for t in range(0, T):\n",
    "        # inherit from last period\n",
    "\n",
    "        # infer sum of capital\n",
    "        K_vec[t] = torch.sum(h_matrix[t,:])\n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec[t] = torch.sum(l_matrix[t,:])\n",
    "\n",
    "        r[t] = interest_rate(K_vec[t], L_vec[t], delta_vec[t], tfp_vec[t], params)\n",
    "        w[t] = wage(K_vec[t], L_vec[t], tfp_vec[t], params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth[t, :] = h_matrix[t,:]*r[t] + l_matrix[t,:]*w[t]\n",
    "\n",
    "        ## savings choice\n",
    "        if use_true_model == True:\n",
    "            a_matrix[t,:] = wealth[t, :]*params.mult_wealth\n",
    "            c_matrix[t,:] = wealth[t, :] - a_matrix[t,:]\n",
    "        else:\n",
    "            ## Conditional expectation\n",
    "            PE_t = model_normalized(neural_net, tfp_vec[t].view(1,-1), wealth[t, :].view(1,-1), h_matrix[t, :].view(1,-1), params)\n",
    "            ## Infer consumption\n",
    "            c_guess = c_guess_function(PE_t, wealth[t, :])\n",
    "            ## Infer capital decision\n",
    "            a_guess = wealth[t, :] - c_guess\n",
    "            ## Apply truncation\n",
    "            a_matrix[t, :] = torch.where(a_guess > 0, a_guess, 0.0)\n",
    "            c_matrix[t, :] = torch.clamp(wealth[t, :] - a_matrix[t, :], tol)\n",
    "            \n",
    "        ## next period\n",
    "        # Sift by one period\n",
    "        if t < (T-1):\n",
    "            h_matrix[t+1,1:] = a_matrix[t,0:-1].clone()\n",
    "      \n",
    "    exo_states = torch.column_stack((tfp_vec, delta_vec))\n",
    "    observations = torch.hstack((h_matrix[burnin:, :], exo_states[burnin:, :])) \n",
    "    \n",
    "    return observations\n",
    "\n",
    "def generate_data_and_dataloaders_current_model(neural_net, initial_state_vector, params, T, batch_size, use_true_model = False, non_stochastic_ss = False):\n",
    "    \"\"\"\n",
    "    Generate data using current neural network. Return a dataloader without and with replacement.\n",
    "    Input:\n",
    "    neural_net: a pytorch neural network\n",
    "    len_T: length of simulation\n",
    "    params: a Params object\n",
    "    use_true_model: if true, use analytic solution. Else, use neural net\n",
    "    non_stochastic_ss: if true, simulation with exo variables constant\n",
    "    \"\"\"\n",
    "    if T < batch_size:\n",
    "        raise Exception(f\"T: {T} < batch_size: {batch_size}\") \n",
    "    observations = simulate_current_model(neural_net, initial_state_vector, T, params, use_true_model, non_stochastic_ss)\n",
    "    dataloader, dataloader_with_replacement = generate_dataloaders(observations, batch_size)\n",
    "    return dataloader, dataloader_with_replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287b3899-d26e-4b06-9fd9-ed099ab55e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_model(len_T, params, neural_net, state_variables_SS, control_variables_SS, A, B, C, D, \n",
    "                   indices_lambdas, indices_assets, use_true_model = False, use_linear = False, \n",
    "                   non_stochastic_ss = False, series_innovation = None, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Use neural network or linearized solution to simulate the model. Can also be used for IRF if series_shock is not empty.\n",
    "\n",
    "    For linearization, uses state-space representation.\n",
    "    State vector:\n",
    "    S_t = A * S_{t-1} + B * e_{t}\n",
    "    Control vector:\n",
    "    X_t = C * S_{t-1} + D * e_{t}\n",
    "    S and e are expressed in deviation from SS values.\n",
    "    \n",
    "    Input:\n",
    "    len_T: length of simulation\n",
    "    params: a Params object\n",
    "    neural_net: a pytorch neural network.\n",
    "    state_variables_SS: SS state variables (can be calculated with Dynare)\n",
    "    A: matrix linearized solution (can be calculated with Dynare)\n",
    "    B: matrix linearized solution (can be calculated with Dynare)\n",
    "    C: matrix linearized solution (can be calculated with Dynare)\n",
    "    D: matrix linearized solution (can be calculated with Dynare)\n",
    "    use_true_model: if true, use exact model, for the special case l_0 = 1 and non-binding borrowing constraint.\n",
    "    use_linear: if true, use linear approximation (uses matrices state_variables_SS, A - D). Else use NN\n",
    "    non_stochastic_ss: if true, simulation with no shocks\n",
    "    series_shock: if not empty, use the series of shocks during simulation\n",
    "    \"\"\"\n",
    "    #add burnin if simulation\n",
    "    if series_innovation is None:\n",
    "        burnin = int(len_T/10) #10% burnin\n",
    "    else:\n",
    "         burnin = 0\n",
    "    T = len_T + burnin\n",
    "    \n",
    "    # random draws\n",
    "    if non_stochastic_ss == False:\n",
    "        # Simulate\n",
    "        if series_innovation is None:\n",
    "            #tfp_vec, delta_vec = simulate_shocks(params, T)\n",
    "            e_tfp, e_delta, tfp_vec, delta_vec = simulate_innovations_and_shocks(params, T)\n",
    "        # Use value provided by user\n",
    "        else:\n",
    "            e_tfp, e_delta = series_innovation[0,:], series_innovation[1,:]\n",
    "            # Series\n",
    "            tfp_vec, delta_vec = simulate_shocks_given_innovations(params, T, e_tfp, e_delta)\n",
    "    else:\n",
    "        tfp_vec = params.mean_tfp*torch.ones(T)\n",
    "        delta_vec = params.mean_delta*torch.ones(T)\n",
    "        e_tfp = torch.zeros_like(tfp_vec)\n",
    "        e_delta =  torch.zeros_like(delta_vec)\n",
    "        \n",
    "    # Get innovation:\n",
    "    innovation_vector = torch.vstack([e_tfp, e_delta])\n",
    "    \n",
    "    #distribution capital holdings:\n",
    "    h_matrix = torch.zeros((T, params.nb_agents))\n",
    "    h_matrix[0,1:] = state_variables_SS[indices_assets] #first period. Start from non-stochastic SS.\n",
    "    K_vec = torch.zeros(T) #aggregate capital\n",
    "    Y_vec = torch.zeros(T) #aggregate output\n",
    "    ## Exogeneous labour supply. One first period, 0, else\n",
    "    l_matrix = torch.zeros((T, params.nb_agents))\n",
    "    l_matrix[:, 0:params.A_tilde+1] = params.l_s #supply ls units of labor from 0 until A_tilde (included).\n",
    "    L_vec = torch.zeros(T)\n",
    "    r = torch.zeros(T)\n",
    "    w = torch.zeros(T)\n",
    "    wealth = torch.zeros((T, params.nb_agents))\n",
    "    # State matrix\n",
    "    S_matrix = torch.zeros((T, params.nb_agents))\n",
    "    S_matrix[0, :] = state_variables_SS \n",
    "    # Assets\n",
    "    a_matrix = torch.zeros((T, params.nb_agents))\n",
    "    a_matrix[0, 0:params.nb_agents-1] = state_variables_SS[indices_assets]\n",
    "    c_matrix = torch.zeros((T, params.nb_agents))\n",
    "    lamb_matrix = torch.zeros((T, params.nb_agents)) #matrix for lagrange multipliers\n",
    "    lamb_matrix[0, 0:params.nb_agents-1] = control_variables_SS[indices_lambdas]\n",
    "    \n",
    "    # Loop over time periods:\n",
    "    for t in range(0, T):\n",
    "        # inherit from last period\n",
    "        # infer sum of capital\n",
    "        K_vec[t] = torch.sum(h_matrix[t,:])\n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec[t] = torch.sum(l_matrix[t,:])\n",
    "        ## Output = torch.zeros(T)\n",
    "        Y_vec[t] = Production(K_vec[t], L_vec[t], tfp_vec[t], params)\n",
    "        r[t] = interest_rate(K_vec[t], L_vec[t], delta_vec[t], tfp_vec[t], params)\n",
    "        w[t] = wage(K_vec[t], L_vec[t], tfp_vec[t], params)\n",
    "    \n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth[t, :] = h_matrix[t,:]*r[t] + l_matrix[t,:]*w[t]\n",
    "    \n",
    "        ## savings choice\n",
    "        # True solution\n",
    "        if use_true_model == True:\n",
    "            a_matrix[t,:] = wealth[t, :]*params.mult_wealth\n",
    "            c_matrix[t,:] = wealth[t, :] - a_matrix[t,:]\n",
    "        # Approximations\n",
    "        else:\n",
    "            # 1st orders\n",
    "            if use_linear == True:\n",
    "                if t>0:\n",
    "                    # Evolution of control variables\n",
    "                    # State vector:\n",
    "                    # S_t = A * S_{t-1} + B * e_{t}\n",
    "                    S_matrix[t, :] = state_variables_SS.view(params.nb_agents, ) + torch.matmul(A, (S_matrix[t-1,:] - state_variables_SS).view(params.nb_agents, )) + torch.matmul(B, innovation_vector[:,t].view(B.shape[1], ))\n",
    "                    # Assets\n",
    "                    a_matrix[t, 0:params.nb_agents-1] = S_matrix[t, indices_assets]\n",
    "                    a_matrix[t, params.nb_agents-1] = 0.0\n",
    "                    #Control vector:\n",
    "                    #X_t = C * S_{t-1} + D * e_{t}\n",
    "                    control_vector = control_variables_SS + torch.matmul(C, (S_matrix[t-1,:] - state_variables_SS).view(params.nb_agents, )) + torch.matmul(D, innovation_vector[:,t].view(B.shape[1], ))\n",
    "                    #Extract lambdas\n",
    "                    lamb_matrix[t, 0:params.nb_agents-1] = control_vector[indices_lambdas]\n",
    "                    lamb_matrix[t, params.nb_agents-1] = 0.0\n",
    "                    \n",
    "                ## Infer consumption decision\n",
    "                c_matrix[t, :] = wealth[t, :] - a_matrix[t, :]\n",
    "            # NN\n",
    "            else:\n",
    "                PE_t = model_normalized(neural_net, tfp_vec[t].view(1,-1), wealth[t, :].view(1,-1), h_matrix[t, :].view(1,-1), params)\n",
    "                ## Infer consumption\n",
    "                c_guess = c_guess_function(PE_t, wealth[t, :]) \n",
    "                ## Infer capital decision\n",
    "                a_guess = wealth[t, :] - c_guess\n",
    "                ## Apply truncation and get implied lambda\n",
    "                a_matrix[t, :] = torch.where(a_guess > 0, a_guess, 0.0)\n",
    "                c_matrix[t, :] = torch.clamp(wealth[t, :] - a_matrix[t, :], tol)\n",
    "    \n",
    "                ## Lagrange multiplier: indicator if binding or not\n",
    "                lamb_matrix[t, 0:params.nb_agents-1] = torch.where(a_matrix[t, 0:params.nb_agents-1] == 0, 1, 0)\n",
    "                \n",
    "        ## next period\n",
    "        # Sift by one period\n",
    "        if t < (T-1):\n",
    "            h_matrix[t+1,1:] = a_matrix[t,0:-1].clone()\n",
    "      \n",
    "    exo_states = torch.column_stack((tfp_vec, delta_vec))\n",
    "    observations = torch.hstack((h_matrix[burnin:, :], exo_states[burnin:, :])) \n",
    "    \n",
    "    # Construct a dataframe\n",
    "    columns = ['t', 'tfp', 'delta', 'K', 'L', 'r', 'w', 'Y']\n",
    "    columns += [f'a_{i}' for i in range(params.nb_agents)]\n",
    "    columns += [f'c_{i}' for i in range(params.nb_agents)]\n",
    "    columns += [f'wealth_{i}' for i in range(params.nb_agents)]\n",
    "    columns += [f'lamb_{i}' for i in range(params.nb_agents)]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        row = [t+1, tfp_vec[t].numpy(), delta_vec[t].numpy(), K_vec[t].numpy(), L_vec[t].numpy(), r[t].numpy(), w[t].numpy(), Y_vec[t].numpy()]\n",
    "        row += list(a_matrix[t, :].numpy())  # Convert tensors to numpy if necessary\n",
    "        row += list(c_matrix[t, :].numpy())\n",
    "        row += list(wealth[t, :].numpy())\n",
    "        row += list(lamb_matrix[t, :].numpy())\n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    return df, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e46afb9-09f6-4dae-a288-de70ac7ab5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_IRF(size_shock, len_IRF, params, neural_net, state_variables_SS, control_variables_SS, A, B, C, D, indices_lambdas, indices_assets, use_true_model = False, use_linear = False, do_plots=False):\n",
    "    \"\"\"\n",
    "    Simulate IRF models. Initialize series at the non-stochastic SS.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # First find SS.\n",
    "        ## No innovation, simulate forward\n",
    "        len_transition_back_SS = 200\n",
    "        innovations = torch.zeros(2, len_transition_back_SS)\n",
    "        df_SS, obs = simulate_model(len_transition_back_SS, params, neural_net, state_variables_SS, control_variables_SS, A, B, C, D, indices_lambdas, indices_assets, use_true_model = use_true_model, use_linear = use_linear, non_stochastic_ss = False, series_innovation = innovations)\n",
    "        ## Starting from SS, simulate transition\n",
    "        list_shocks = [\"TFP\", \"Depreciation\"]\n",
    "        list_dfs = []\n",
    "    \n",
    "        # Extract assets SS:\n",
    "        pattern = re.compile(r'^a_\\d+$')\n",
    "        # Remove last column (a_N)\n",
    "        SS_local = torch.from_numpy(np.hstack((df_SS['tfp'].values[-1], df_SS.filter(regex=pattern).iloc[:,:-1].values[-1])))\n",
    "        \n",
    "        for s in list_shocks:\n",
    "            innovations = torch.zeros(2, len_IRF)\n",
    "            if s == \"TFP\":\n",
    "                # Shock to TFP\n",
    "                list_vars = [\"tfp\", \"delta\", \"K\", \"Y\", \"r\", \"w\"] \n",
    "                innovations[0, 1] = size_shock*params.std_tfp \n",
    "            elif s==\"Depreciation\":\n",
    "                list_vars = [\"tfp\", \"delta\", \"K\", \"Y\", \"r\", \"w\"] \n",
    "                innovations[1, 1] = size_shock*params.std_delta\n",
    "            else:\n",
    "                raise TypeError(\"shock type is unknown\")\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # Start from the SS calculated above\n",
    "                df, obs = simulate_model(len_IRF, params, neural_net, SS_local, control_variables_SS, A, B, C, D, indices_lambdas, indices_assets, use_true_model = use_true_model, use_linear = use_linear, non_stochastic_ss = False, series_innovation = innovations)\n",
    "                list_dfs.append(df)\n",
    "            if do_plots == True:\n",
    "                fig, axs = plt.subplots(2, 3)\n",
    "                for var, ax in zip(list_vars, axs.flat):\n",
    "                    ax.plot(df[var], label=var)\n",
    "                    ax.legend()\n",
    "                plt.suptitle(f\"One std. dev {s} shock\")\n",
    "                plt.show()\n",
    "    return list_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd29756e-bed1-4e3e-b9c1-8e08d01e21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot descriptive stats by age\n",
    "def plot_linearized_model(df_linearized, params, name_fig=None):\n",
    "    \"\"\"\n",
    "    Function to plot the linearized model\n",
    "    \"\"\"\n",
    "    df_h = df_linearized.filter(regex=r'^wealth_\\d+$')\n",
    "    df_a = df_linearized.filter(regex=r'^a_\\d+$')\n",
    "    df_lambda = df_linearized.filter(regex=r'^lamb_\\d+$')\n",
    "    df_c = df_linearized.filter(regex=r'^c_\\d+$')\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    age_vec = np.arange(params.nb_agents) + 1\n",
    "    line_styles = ['-', '--', ':', '-.', 'dashdot']\n",
    "    colors = sns.color_palette(\"Set2\", 4)\n",
    "    \n",
    "    a_mean = df_a.mean()\n",
    "    a_p10 = df_a.quantile(0.1)\n",
    "    a_p90 = df_a.quantile(0.9)\n",
    "    a_min = df_a.min()\n",
    "    \n",
    "    c_mean = df_c.mean()\n",
    "    c_p10 = df_c.quantile(0.1)\n",
    "    c_p90 = df_c.quantile(0.9)\n",
    "    c_min = df_c.min()\n",
    "    \n",
    "    lamb_mean = df_lambda.mean()\n",
    "    lamb_p10 = df_lambda.quantile(0.1)\n",
    "    lamb_p90 = df_lambda.quantile(0.9)\n",
    "    lamb_min = df_lambda.min()\n",
    "    \n",
    "    wealth_mean = df_h.mean()\n",
    "    wealth_p10 = df_h.quantile(0.1)\n",
    "    wealth_p90 = df_h.quantile(0.9)\n",
    "    wealth_min = df_h.min()\n",
    "    \n",
    "    #ax1.scatter(age_vec, a_mean)\n",
    "    ax1.plot(age_vec, a_mean, label=\"Mean\", color = colors[0], linestyle = line_styles[0])\n",
    "    ax1.plot(age_vec, a_p10, label=\"P10\", color = colors[1], linestyle = line_styles[1])\n",
    "    ax1.plot(age_vec, a_p90, label=\"P90\", color = colors[2], linestyle = line_styles[2])\n",
    "    #ax1.plot(age_vec, a_mean_theory, label=\"mean analytic (l_0 = 1)\")\n",
    "    ax1.axvline(x = params.A_tilde+1, color = 'black', linestyle = line_styles[3], alpha=0.5)\n",
    "    ax1.plot(age_vec, a_min, label=\"Minimum\", color = colors[3], linestyle = line_styles[4])\n",
    "    ax1.set_ylabel(\"Capital\")\n",
    "    ax1.set_xlabel(\"Age\")\n",
    "    ax1.set_xticks(range(1, params.nb_agents+1))\n",
    "    ax1.legend()\n",
    "    \n",
    "    #ax2.scatter(age_vec, c_mean, label=\"mean consumption\")\n",
    "    ax2.plot(age_vec, c_mean, label=\"mean\", color = colors[0],linestyle = line_styles[0])\n",
    "    ax2.plot(age_vec, c_p10, label=\"P10\", color = colors[1], linestyle = line_styles[1])\n",
    "    ax2.plot(age_vec, c_p90, label=\"P90\", color = colors[2], linestyle = line_styles[2])\n",
    "    #ax2.plot(age_vec, c_mean_theory, label=\"mean analytic (l_0 = 1)\")\n",
    "    ax2.axvline(x = params.A_tilde+1, color = 'black', linestyle = '-.', alpha=0.5)\n",
    "    ax2.plot(age_vec, c_min, label=\"Minimum\", color = colors[3], linestyle = line_styles[4])\n",
    "    ax2.set_ylabel(\"Consumption\")\n",
    "    ax2.set_xlabel(\"Age\")\n",
    "    ax2.set_xticks(range(1, params.nb_agents+1))\n",
    "    #ax2.legend()\n",
    "    \n",
    "    \n",
    "    ax3.plot(age_vec, lamb_mean, label=\"mean\", color = colors[0], linestyle = line_styles[0])\n",
    "    ax3.plot(age_vec, lamb_p10, label=\"P10\", color = colors[1], linestyle = line_styles[1])\n",
    "    ax3.plot(age_vec, lamb_p90, label=\"P90\", color = colors[2], linestyle = line_styles[2])\n",
    "    ax3.axvline(x = params.A_tilde+1, color = 'black', linestyle = '-.', alpha=0.5)\n",
    "    ax3.plot(age_vec, lamb_min, label=\"Minimum\", color = colors[3], linestyle = line_styles[4])\n",
    "    ax3.set_ylabel(\"Lagrange mult.\")\n",
    "    ax3.set_xlabel(\"Age\")\n",
    "    ax3.set_xticks(range(1, params.nb_agents+1))\n",
    "    #ax3.legend()\n",
    "    \n",
    "    ax4.plot(age_vec, wealth_mean, label=\"mean\", color = colors[0], linestyle = line_styles[0])\n",
    "    ax4.plot(age_vec, wealth_p10, label=\"P10\", color = colors[1], linestyle = line_styles[1])\n",
    "    ax4.plot(age_vec, wealth_p90, label=\"P90\", color = colors[2], linestyle = line_styles[2])\n",
    "    ax4.axvline(x = params.A_tilde+1, color = 'black', linestyle = '-.', alpha=0.5)\n",
    "    ax4.plot(age_vec, wealth_min, label=\"Minimum\", color = colors[3], linestyle = line_styles[4])\n",
    "    ax4.set_ylabel(\"Wealth\")\n",
    "    ax4.set_xlabel(\"Age\")\n",
    "    ax4.set_xticks(range(1, params.nb_agents+1))\n",
    "    #ax4.legend()\n",
    "    plt.tight_layout()\n",
    "    if name_fig != None:\n",
    "        plt.savefig(name_fig, bbox_inches=\"tight\", dpi=600)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c69ba",
   "metadata": {},
   "source": [
    "## bc-MC Operator\n",
    "\n",
    "For a single age category, the bc-MC operator is:\n",
    "\n",
    "$$ \\frac{1}{M} \\frac{2}{(N)(N-1)} \\sum_{m=1}^{M} \\sum_{1\\leq i < j}^{n} f(s_m, \\epsilon_{m}^{(i)})f(s_m, \\epsilon_{m}^{(j)})  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a6c7f",
   "metadata": {},
   "source": [
    "### To measure the accuracy of the bc-MC operator\n",
    "\n",
    "\n",
    "#### Monte Carlo integration\n",
    "\n",
    "We want to calculate the mean value of the Euler equation error (EEE):\n",
    "\n",
    "$$EEE =  \\frac{1}{c_t}(u^{\\prime-1})\\Big(\\mathbf{E}_{\\varepsilon}\\big[{\\beta u^{\\prime}(c_{t+1}) r_{t+1}} \\big]\\Big) - 1$$\n",
    "\n",
    "Let's first use Monte Carlo to approximate the integral with respect to the innovation vector. Let's first fix the value of the state vector to $s_m$. Conditional on this value, the expectation with respect to the innovation vector is approximated by:\n",
    "\n",
    "$$ \\mathbf{E}_{\\varepsilon} g(s_m,  \\epsilon) \\approx \\frac{1}{N} \\sum_{i=1}^{N}  g(s_m,  \\epsilon^{(i)}) $$\n",
    "\n",
    "wich can be vectorized as\n",
    "\n",
    "$$ \\mathbf{1}_N^{T} . \\begin{pmatrix} g(s_m, \\epsilon^{(1)}) \\\\ \\vdots \\\\ g(s_m, \\epsilon^{(N)}) \\end{pmatrix} $$\n",
    "\n",
    "where $\\mathbf{1}_N^{T} = (\\frac{1}{n}, \\frac{1}{n}, ..., \\frac{1}{n})$ is a $(N, 1)$ row vector.\n",
    "\n",
    "Now, for several draws of $s_m$, we can calculate conditional means as:\n",
    "\n",
    "$$ \\begin{pmatrix} \\frac{1}{N} \\sum_{i=1}^{N}  g(s_1,  \\epsilon^{(i)}) \\\\ \\vdots \\\\ \\frac{1}{N} \\sum_{i=1}^{N}  g(s_m,  \\epsilon^{(i)})) \\end{pmatrix} = \\begin{pmatrix}\n",
    "\\mathbf{1}_N^{T} & \\mathbf{0} & ... & \\mathbf{0}\\\\\n",
    "\\mathbf{0} & \\mathbf{1}_N^{T} & \\mathbf{0} & \\mathbf{0} \\\\\n",
    "... & \\mathbf{0} & ... & ... \\\\\n",
    "\\mathbf{0} & \\mathbf{0} & ... & \\mathbf{1}_N^{T}\n",
    "\\end{pmatrix}  \\begin{pmatrix} g(s_1,  \\epsilon_1^{(i)}) \\\\ \\vdots \\\\ g(s_1,  \\epsilon_1^{(N)}) \\\\ \\vdots \\\\ g(s_M,  \\epsilon_M^{(1)}) \\\\ ... \\\\ g(s_M,  \\epsilon_M^{(N)}) \\end{pmatrix}$$\n",
    "\n",
    "This is what I do in the function `evaluate_accuracy_pytorch_MC` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30ce9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_pytorch_MC(neural_net, n_target, n_Monte_Carlo, params, dataloader_replacement, \n",
    "                                 debug=False, truncate_a = True, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Monte Carlo for the expectations\n",
    "    \"\"\"\n",
    "    # n: number of draws for current state\n",
    "    # n_Monte_Carlo: number of draws for next state\n",
    "    ## Convert to multiple of params.M\n",
    "    n = int(np.ceil(n_target/params.M)*params.M)\n",
    "    with torch.no_grad():\n",
    "        # To calculate means quickly, vectorize\n",
    "        # Sparse version\n",
    "        A = sparse.eye(n) #(n,n) identity matrix\n",
    "        B = sparse.csr_matrix(np.ones(n_Monte_Carlo)/n_Monte_Carlo) #(1, n_Monte_Carlo) row vector\n",
    "        # Sparse kronecker product. Then convert to pytorch sparse.\n",
    "        D = sparse.kron(A, B) #(n, n_Monte_Carlo) matrix, with repeated row vectors B. \n",
    "        # Repeat A-1 times\n",
    "        rep = params.nb_agents - 1\n",
    "        I_rep = sparse.csr_matrix(np.eye(rep, rep))\n",
    "        D_repeated = sparse.kron(I_rep, D)\n",
    "        # Convert to sparse tensor\n",
    "        W = sparse_mx_to_torch_sparse_tensor(D_repeated)\n",
    "    \n",
    "        # State vector\n",
    "        ## Get the numer batch size necessary to get n draws\n",
    "        if n < params.M:\n",
    "            nb_draws = params.M\n",
    "        else:\n",
    "            nb_draws = int(n/params.M)\n",
    "        state_vec = generate_n_batches(nb_draws, dataloader_replacement) #each draw is of size M\n",
    "        # Select right size\n",
    "        state_vec = state_vec[:n,:]\n",
    "        \n",
    "        #print(state_vec.shape)\n",
    "        h_matrix = state_vec[:,:-2] \n",
    "        # current value for z and delta\n",
    "        tfp_vec = state_vec[:, -2]\n",
    "        delta_vec = state_vec[:, -1]\n",
    "        \n",
    "        ## Innovation vector\n",
    "        # n_Monte_Carlo for each value today\n",
    "        e_tfp, e_delta = simulate_innovations(params, n*n_Monte_Carlo) #simulate_shocks(params, n*n_Monte_Carlo)\n",
    "        innovation_vec =  torch.column_stack((e_tfp, e_delta))\n",
    "    \n",
    "        ## Current period\n",
    "        # infer sum of capital\n",
    "        K_vec = torch.sum(h_matrix, 1)\n",
    "\n",
    "        ## Exogeneous labour supply. \n",
    "        l_matrix = torch.zeros_like(h_matrix)\n",
    "        l_matrix[:, 0:params.A_tilde+1] = params.l_s #supply ls units of labor from 0 until A_tilde (included).\n",
    "        \n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec = torch.sum(l_matrix, 1)\n",
    "\n",
    "        r = interest_rate(K_vec, L_vec, delta_vec, tfp_vec, params)\n",
    "        w = wage(K_vec, L_vec, tfp_vec, params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth = h_matrix*r.view(-1,1) + l_matrix*w.view(-1,1)\n",
    "\n",
    "        ## Consumption curent period\n",
    "        if debug == False:\n",
    "            PE_t = model_normalized(neural_net, tfp_vec, wealth, h_matrix, params)\n",
    "            ## Infer consumption\n",
    "            c_guess = c_guess_function(PE_t, wealth)\n",
    "            ## Infer capital decision\n",
    "            a_guess = wealth - c_guess\n",
    "            ## Truncation\n",
    "            if truncate_a==True:\n",
    "                a = torch.where(a_guess > 0, a_guess, 0.0)\n",
    "                c = torch.clamp(wealth - a, tol)\n",
    "            else:\n",
    "                a = a_guess\n",
    "                c = c_guess\n",
    "            ## Dummy lagrange multiplier\n",
    "            lamb = torch.zeros_like(c)\n",
    "        else:\n",
    "            a = wealth*params.mult_wealth.view(1, -1)\n",
    "            c = wealth - a\n",
    "            lamb = torch.zeros_like(c)\n",
    "       \n",
    "        ## Period t+1\n",
    "        ## comes from last period. But first generation has zero capital\n",
    "        h_matrix_next = torch.zeros_like(a)\n",
    "        # Sift by one period\n",
    "        h_matrix_next[:,1:] = a[:,0:-1].clone()\n",
    "        h_matrix_next = h_matrix_next.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "\n",
    "        ## Repeat values from period t to vectorize code\n",
    "        c_repeated = c.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        a_repeated = a.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        lamb_repeated = lamb.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        tfp_vec_repeated = tfp_vec.repeat_interleave(n_Monte_Carlo, dim=0)\n",
    "        delta_vec_repeated = delta_vec.repeat_interleave(n_Monte_Carlo, dim=0)\n",
    "        \n",
    "        # transitions of the exogenous processes\n",
    "        ## No need to repeat. Already rigth shape\n",
    "        tfp_vec_next = torch.exp(params.rho_tfp*torch.log(tfp_vec_repeated) + e_tfp) # innovation_vec[:, -2]\n",
    "        delta_vec_next = delta_vec_repeated + e_delta #innovation_vec[:, -1]\n",
    "\n",
    "        K_vec_next = torch.sum(h_matrix_next, 1)\n",
    "\n",
    "        ## Exogeneous labour supply. \n",
    "        l_matrix_next = torch.zeros_like(h_matrix_next)\n",
    "        l_matrix_next[:, 0:params.A_tilde+1] = params.l_s #supply ls units of labor from 0 until A_tilde (included).\n",
    "        \n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec_next = torch.sum(l_matrix_next, 1)\n",
    "\n",
    "        ## get factor prices (wages and interest rate)\n",
    "        r_next = interest_rate(K_vec_next, L_vec_next, delta_vec_next, tfp_vec_next, params)\n",
    "        w_next = wage(K_vec_next, L_vec_next, tfp_vec_next, params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth_next = h_matrix_next*r_next.view(-1,1) + l_matrix_next*w_next.view(-1,1)\n",
    "\n",
    "        ## Consumption curent period\n",
    "        if debug == False:\n",
    "            PE_next = model_normalized(neural_net, tfp_vec_next, wealth_next, h_matrix_next, params)\n",
    "            ## Infer consumption\n",
    "            c_next_guess = c_guess_function(PE_next, wealth_next)\n",
    "            ## Infer capital decision\n",
    "            a_next_guess = wealth_next - c_next_guess\n",
    "            ## Truncation\n",
    "            if truncate_a==True:\n",
    "                a_next = torch.where(a_next_guess > 0, a_next_guess, 0.0)\n",
    "                c_next = torch.clamp(wealth_next - a_next, tol)\n",
    "            else:\n",
    "                a_next = a_next_guess\n",
    "                c_next = c_next_guess\n",
    "            ## Dummy value\n",
    "            lamb_next = torch.zeros_like(c_next)\n",
    "        else:\n",
    "            a_next = wealth_next*params.mult_wealth.view(1, -1)\n",
    "            c_next = wealth_next - a_next\n",
    "            lamb_next =  torch.zeros_like(c_next)\n",
    "\n",
    "        # Each column is the euler equation for one agent\n",
    "        # rows are observations\n",
    "        #s = c_next[:, 1:params.nb_agents].shape\n",
    "        #print(f\"shape c next: {s}\")\n",
    "        u_prime_next = params.u_prime(c_next)\n",
    "        #u_prime_next = c_next**(-params.gamma) \n",
    "\n",
    "        # Calculate beta (u'-1){E[beta*u'(c^{h+1}_{t+1})*r_{t+1} + lambda^{h}_{t}]}\n",
    "        vals = u_prime_next[:, 1:params.nb_agents]*r_next.view(-1,1) \n",
    "        #print(vals.shape)\n",
    "        # Reshape matrix (MN, nb_agents) to a single column array of size (nb_agents*MN, 1)\n",
    "        # First column, then second column, the third column, and so on..\n",
    "        vals_reshaped = vals.t().contiguous().view(-1, 1)\n",
    "        \n",
    "       \n",
    "        #print(W.shape)\n",
    "        #torch.sparse.mm(W, vals_reshaped)\n",
    "        #\n",
    "        #print(wealth_reshaped.shape)\n",
    "        #print(vals_reshaped.shape)\n",
    "        # Expectation\n",
    "        Expectation = params.beta*torch.sparse.mm(W, vals_reshaped)\n",
    "        # See for instance: https://julia.quantecon.org/dynamic_programming/ifp.html\n",
    "        ## When non-binding: usual Euler. Otherwise, savings = 0\n",
    "        wealth_reshaped = wealth[:, 0:params.nb_agents-1].t().contiguous().view(-1, 1)\n",
    "        max_two_values = torch.maximum(Expectation, params.u_prime(wealth_reshaped))**(-1.0/params.gamma)\n",
    "\n",
    "        # Euler equation error\n",
    "        c_reshaped = c[:, 0:params.nb_agents-1].t().contiguous().view(-1, 1)\n",
    "        #lamb_reshaped = c[:, 0:params.nb_agents-1].t().contiguous().view(-1, 1)\n",
    "        \n",
    "        EEE = (max_two_values/c_reshaped) - 1 # have to add lagrange mult\n",
    "        \n",
    "    return EEE.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10055b",
   "metadata": {},
   "source": [
    "#### Gaussian quadrature\n",
    "\n",
    "We can also use [Gaussian quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature) to approximate the integral with respect to the innovation vector. \n",
    "Once again, let's first fix the value of the state vector to $s_m$. Conditional on this value, the expectation with respect to the innovation vector is approximated by:\n",
    "\n",
    "$$ \\mathbf{E}_{\\varepsilon} g(s_m,  \\epsilon) \\approx \\sum_{i=1}^{N} w_{i}  g(s_m,  \\epsilon^{(i)}) $$\n",
    "\n",
    "If we set $w_i = \\frac{1}{n}$ and if we use random and independent draws for $\\epsilon^{(i)}$, we are back to the Monte Carlo case discussed above. This observation makes us realize that we can reuse the same vectorization scheme as above, with minor modifications. More specifically, conditional on a given value for $s_m$, the expectation with respect to the innovation vector is approximated by:\n",
    "\n",
    "$$ \\mathbf{E}_{\\varepsilon} g(s_m,  \\epsilon) \\approx \\sum_{i=1}^{N} w_i g(s_m,  \\epsilon^{(i)}) $$\n",
    "\n",
    "with $w_i$ Gaussian quadrature weights and $\\epsilon^{(i)}$ the corresponding Gaussian quadrature nodes.\n",
    "\n",
    "This can be vectorized as\n",
    "\n",
    "$$ \\mathbf{1}_{w}^{T} . \\begin{pmatrix} g(s_m, \\epsilon^{(1)}) \\\\ \\vdots \\\\ g(s_m, \\epsilon^{(N)}) \\end{pmatrix} $$\n",
    "\n",
    "where $\\mathbf{1}_w^{T} = (w_1, w_2, ..., w_N)$ is a $(N, 1)$ row vector.\n",
    "\n",
    "Now, for several draws of $s_m$, we can calculate conditional means as:\n",
    "\n",
    "$$ \\begin{pmatrix} \\frac{1}{N} \\sum_{i=1}^{N}  g(s_1,  \\epsilon^{(i)}) \\\\ \\vdots \\\\ \\frac{1}{N} \\sum_{i=1}^{N}  g(s_m,  \\epsilon^{(i)})) \\end{pmatrix} = \\begin{pmatrix}\n",
    "\\mathbf{1}_{w}^{T} & \\mathbf{0} & ... & \\mathbf{0}\\\\\n",
    "\\mathbf{0} & \\mathbf{1}_{w}^{T} & \\mathbf{0} & \\mathbf{0} \\\\\n",
    "... & \\mathbf{0} & ... & ... \\\\\n",
    "\\mathbf{0} & \\mathbf{0} & ... & \\mathbf{1}_{w}^{T}\n",
    "\\end{pmatrix}  \\begin{pmatrix} g(s_1,  \\epsilon_1^{(i)}) \\\\ \\vdots \\\\ g(s_1,  \\epsilon_1^{(N)}) \\\\ \\vdots \\\\ g(s_M,  \\epsilon_M^{(1)}) \\\\ ... \\\\ g(s_M,  \\epsilon_M^{(N)}) \\end{pmatrix}$$\n",
    "\n",
    "This is what I do in the function `evaluate_accuracy_pytorch_Gaussian` below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7e6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_pytorch_Gaussian(neural_net, n_target, params, dataloader_replacement, debug=False, \n",
    "                                       truncate_a = True, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Function to evaluate the accuracy using Gaussian quadrature for the expectations\n",
    "    Use new draws at each call\n",
    "    \"\"\"\n",
    "    # n: number of draws for current state\n",
    "    ## Convert to multiple of params.M\n",
    "    n = int(np.ceil(n_target/params.M)*params.M)\n",
    "    with torch.no_grad():\n",
    "        # To repeat vectors\n",
    "        n_Monte_Carlo = len(params.weights) #length of nodes\n",
    "        \n",
    "        # To calculate means quickly, vectorize\n",
    "        # Sparse version\n",
    "        A = sparse.eye(n)\n",
    "        B = sparse.csr_matrix(params.weights)\n",
    "        # Sparse kronecker product. Then convert to pytorch sparse\n",
    "        D = sparse.kron(A, B)\n",
    "        \n",
    "        # Repeat A-1 times\n",
    "        rep = params.nb_agents - 1\n",
    "        I_rep = sparse.csr_matrix(np.eye(rep, rep))\n",
    "        D_repeated = sparse.kron(I_rep, D)\n",
    "        # Convert to sparse tensor\n",
    "        W = sparse_mx_to_torch_sparse_tensor(D_repeated)\n",
    "        \n",
    "        # State vector\n",
    "        ## Get the numer batch size necessary to get n draws\n",
    "        if n < params.M:\n",
    "            nb_draws = params.M\n",
    "        else:\n",
    "            nb_draws = int(n/params.M)\n",
    "        state_vec = generate_n_batches(nb_draws, dataloader_replacement) #each draw is of size M\n",
    "        # Select right size\n",
    "        state_vec = state_vec[:n,:]\n",
    "        \n",
    "        #print(state_vec.shape)\n",
    "        h_matrix = state_vec[:,:-2] \n",
    "        # current value for z and delta\n",
    "        tfp_vec = state_vec[:, -2]\n",
    "        delta_vec = state_vec[:, -1]\n",
    "          \n",
    "        # Innovation vector\n",
    "        #e_r = params.nodes_torch[:,0].float().repeat(n)\n",
    "        #e_δ = params.nodes_torch[:,1].float().repeat(n)\n",
    "        #e_ps = params.nodes_torch[:,2:].float().repeat(n, 1) #e_p1, ..., e_p2\n",
    "\n",
    "        ## Innovation vector\n",
    "        # n_Monte_Carlo for each value today\n",
    "        #e_tfp, e_delta = simulate_shocks(params, n*n_Monte_Carlo)\n",
    "        ## Gaussian quadrature nodes\n",
    "        ## repeat to match the number of draws for the state vector\n",
    "        innovation_vec = params.nodes_torch.float().repeat(n, 1)\n",
    "        e_tfp, e_delta = innovation_vec[:,0], innovation_vec[:,1]\n",
    "        \n",
    "        ## Current period\n",
    "        # infer sum of capital\n",
    "        K_vec = torch.sum(h_matrix, 1)\n",
    "\n",
    "        ## Exogeneous labour supply.\n",
    "        l_matrix = torch.zeros_like(h_matrix)\n",
    "        l_matrix[:, 0:params.A_tilde+1] = params.l_s #supply ls units of labor from 0 until A_tilde (included).\n",
    "        \n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec = torch.sum(l_matrix, 1)\n",
    "\n",
    "        r = interest_rate(K_vec, L_vec, delta_vec, tfp_vec, params)\n",
    "        w = wage(K_vec, L_vec, tfp_vec, params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth = h_matrix*r.view(-1,1) + l_matrix*w.view(-1,1)\n",
    "\n",
    "        ## Consumption curent period\n",
    "        if debug == False:\n",
    "            PE_t = model_normalized(neural_net, tfp_vec, wealth, h_matrix, params)\n",
    "            ## Infer consumption\n",
    "            c_guess = c_guess_function(PE_t, wealth)\n",
    "            ## Infer capital decision\n",
    "            a_guess = wealth - c_guess\n",
    "            ## Truncation\n",
    "            if truncate_a == True:\n",
    "                a = torch.where(a_guess > 0, a_guess, 0.0)\n",
    "                c = torch.clamp(wealth - a, tol)\n",
    "            else:\n",
    "                a = a_guess\n",
    "                c = c_guess\n",
    "            ## Dummy lagrange multiplier\n",
    "            lamb = torch.zeros_like(c)\n",
    "        else:\n",
    "            a = wealth*params.mult_wealth.view(1, -1)\n",
    "            c = wealth - a\n",
    "            ## Dummy lagrange multiplier\n",
    "            lamb = torch.zeros_like(c)\n",
    "       \n",
    "        ## Period t+1\n",
    "        ## comes from last period. But first generation has zero capital\n",
    "        h_matrix_next = torch.zeros_like(a)\n",
    "        # Sift by one period\n",
    "        h_matrix_next[:,1:] = a[:,0:-1].clone()\n",
    "        h_matrix_next = h_matrix_next.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "\n",
    "        ## Repeat values from period t to vectorize code\n",
    "        c_repeated = c.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        a_repeated = a.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        lamb_repeated = lamb.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        tfp_vec_repeated = tfp_vec.repeat_interleave(n_Monte_Carlo, dim=0)\n",
    "        delta_vec_repeated = delta_vec.repeat_interleave(n_Monte_Carlo, dim=0)\n",
    "        \n",
    "        # transitions of the exogenous processes\n",
    "        tfp_vec_next = torch.exp(params.rho_tfp*torch.log(tfp_vec_repeated) + e_tfp) # innovation_vec[:, -2]\n",
    "        delta_vec_next = delta_vec_repeated + e_delta #innovation_vec[:, -1]\n",
    "\n",
    "        K_vec_next = torch.sum(h_matrix_next, 1)\n",
    "\n",
    "        ## Exogeneous labour supply. \n",
    "        l_matrix_next = torch.zeros_like(h_matrix_next)\n",
    "        l_matrix_next[:, 0:params.A_tilde+1] = params.l_s #supply ls units of labor from 0 until A_tilde (included).\n",
    "        \n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec_next = torch.sum(l_matrix_next, 1)\n",
    "\n",
    "        ## get factor prices (wages and interest rate)\n",
    "        r_next = interest_rate(K_vec_next, L_vec_next, delta_vec_next, tfp_vec_next, params)\n",
    "        w_next = wage(K_vec_next, L_vec_next, tfp_vec_next, params)\n",
    "\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth_next = h_matrix_next*r_next.view(-1,1) + l_matrix_next*w_next.view(-1,1)\n",
    "\n",
    "        ## Consumption curent period\n",
    "        if debug == False:\n",
    "            PE_next = model_normalized(neural_net, tfp_vec_next, wealth_next, h_matrix_next, params)\n",
    "            ## Infer consumption\n",
    "            c_next_guess = c_guess_function(PE_next, wealth_next)\n",
    "            ## Infer capital decision\n",
    "            a_next_guess = wealth_next - c_next_guess\n",
    "            ## Truncation\n",
    "            if truncate_a == True:\n",
    "                a_next = torch.where(a_next_guess > 0, a_next_guess, 0.0)\n",
    "                c_next = torch.clamp(wealth_next - a_next, tol)\n",
    "            else:\n",
    "                a_next = a_next_guess\n",
    "                c_next = c_next_guess\n",
    "            ## Dummy value\n",
    "            lamb_next = torch.zeros_like(c_next)\n",
    "        else:\n",
    "            a_next = wealth_next*params.mult_wealth.view(1, -1)\n",
    "            c_next = wealth_next - a_next\n",
    "\n",
    "        # Each column is the euler equation for one agent\n",
    "        # rows are observations\n",
    "        #s = c_next[:, 1:params.nb_agents].shape\n",
    "        #print(f\"shape c next: {s}\")\n",
    "        u_prime_next = params.u_prime(c_next)\n",
    "\n",
    "        # Calculate beta (u'-1){E[beta*u'(c^{h+1}_{t+1})*r_{t+1} + lambda^{h}_{t}]}\n",
    "        vals = u_prime_next[:, 1:params.nb_agents]*r_next.view(-1,1)\n",
    "        \n",
    "        #print(vals.shape)\n",
    "        # Reshape matrix (MN, nb_agents) to a single column array of size (nb_agents*MN, 1)\n",
    "        # First column, then second column, the third column, and so on..\n",
    "        vals_reshaped = vals.t().contiguous().view(-1, 1)\n",
    "        # Expectation\n",
    "        Expectation = params.beta*torch.sparse.mm(W, vals_reshaped)\n",
    "        # See for instance: https://julia.quantecon.org/dynamic_programming/ifp.html\n",
    "        ## When non-binding: usual Euler. Otherwise, savings = 0\n",
    "        wealth_reshaped = wealth[:, 0:params.nb_agents-1].t().contiguous().view(-1, 1)\n",
    "        max_two_values = torch.maximum(Expectation, params.u_prime(wealth_reshaped))**(-1.0/params.gamma)\n",
    "\n",
    "        # Euler equation error\n",
    "        c_reshaped = c[:, 0:params.nb_agents-1].t().contiguous().view(-1, 1)\n",
    "        EEE = (max_two_values/c_reshaped) - 1\n",
    "        \n",
    "    return EEE.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1dcb3e7-1716-4cdb-864a-759f75c1258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EEE_by_age_Gaussian(neural_net, n_target, params, dataloader_replacement, do_plots=False, name_fig=None, \n",
    "                        debug=False, truncate_a = True, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Euler Equation Error by age.\n",
    "    Use Gaussian quadrature for the expectations\n",
    "    Use new draws at each call\n",
    "    \"\"\"\n",
    "    # n_target: number of draws for current state\n",
    "    ## Convert to multiple of params.M\n",
    "    n = int(np.ceil(n_target/params.M)*params.M)\n",
    "    with torch.no_grad():\n",
    "        # To repeat vectors\n",
    "        n_Monte_Carlo = len(params.weights) #length of nodes\n",
    "        \n",
    "        # To calculate means quickly, vectorize\n",
    "        # Sparse version\n",
    "        A = sparse.eye(n)\n",
    "        B = sparse.csr_matrix(params.weights)\n",
    "        # Sparse kronecker product. Then convert to pytorch sparse\n",
    "        D = sparse.kron(A, B)\n",
    "        D_sparse = sparse_mx_to_torch_sparse_tensor(D)\n",
    "                \n",
    "        # State vector\n",
    "        ## Get the numer batch size necessary to get n draws\n",
    "        if n < params.M:\n",
    "            nb_draws = params.M\n",
    "        else:\n",
    "            nb_draws = int(n/params.M)\n",
    "        state_vec = generate_n_batches(nb_draws, dataloader_replacement) #each draw is of size M\n",
    "        # Select right size\n",
    "        state_vec = state_vec[:n,:]\n",
    "        \n",
    "        #print(state_vec.shape)\n",
    "        h_matrix = state_vec[:,:-2] \n",
    "        # current value for z and delta\n",
    "        tfp_vec = state_vec[:, -2]\n",
    "        delta_vec = state_vec[:, -1]\n",
    "          \n",
    "        # Innovation vector\n",
    "        #e_r = params.nodes_torch[:,0].float().repeat(n)\n",
    "        #e_δ = params.nodes_torch[:,1].float().repeat(n)\n",
    "        #e_ps = params.nodes_torch[:,2:].float().repeat(n, 1) #e_p1, ..., e_p2\n",
    "    \n",
    "        ## Innovation vector\n",
    "        # n_Monte_Carlo for each value today\n",
    "        #e_tfp, e_delta = simulate_shocks(params, n*n_Monte_Carlo)\n",
    "        ## Gaussian quadrature nodes\n",
    "        ## repeat to match the number of draws for the state vector\n",
    "        innovation_vec = params.nodes_torch.float().repeat(n, 1)\n",
    "        e_tfp, e_delta = innovation_vec[:,0], innovation_vec[:,1]\n",
    "        \n",
    "        ## Current period\n",
    "        # infer sum of capital\n",
    "        K_vec = torch.sum(h_matrix, 1)\n",
    "    \n",
    "        ## Exogeneous labour supply.\n",
    "        l_matrix = torch.zeros_like(h_matrix)\n",
    "        l_matrix[:, 0:params.A_tilde+1] = params.l_s #supply ls units of labor from 0 until A_tilde (included).\n",
    "        \n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec = torch.sum(l_matrix, 1)\n",
    "    \n",
    "        r = interest_rate(K_vec, L_vec, delta_vec, tfp_vec, params)\n",
    "        w = wage(K_vec, L_vec, tfp_vec, params)\n",
    "    \n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth = h_matrix*r.view(-1,1) + l_matrix*w.view(-1,1)\n",
    "    \n",
    "        ## Consumption curent period\n",
    "        if debug == False:\n",
    "            PE_t = model_normalized(neural_net, tfp_vec, wealth, h_matrix, params)\n",
    "            ## Infer consumption\n",
    "            c_guess = c_guess_function(PE_t, wealth)\n",
    "            ## Infer capital decision\n",
    "            a_guess = wealth - c_guess\n",
    "            ## Truncation\n",
    "            if truncate_a == True:\n",
    "                a = torch.where(a_guess > 0, a_guess, 0.0)\n",
    "                c = torch.clamp(wealth - a, tol)\n",
    "            else:\n",
    "                a = a_guess\n",
    "                c = c_guess\n",
    "            ## Dummy lagrange multiplier\n",
    "            lamb = torch.zeros_like(c)\n",
    "        else:\n",
    "            a = wealth*params.mult_wealth.view(1, -1)\n",
    "            c = wealth - a\n",
    "            ## Dummy lagrange multiplier\n",
    "            lamb = torch.zeros_like(c)\n",
    "       \n",
    "        ## Period t+1\n",
    "        ## comes from last period. But first generation has zero capital\n",
    "        h_matrix_next = torch.zeros_like(a)\n",
    "        # Sift by one period\n",
    "        h_matrix_next[:,1:] = a[:,0:-1].clone()\n",
    "        h_matrix_next = h_matrix_next.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "    \n",
    "        ## Repeat values from period t to vectorize code\n",
    "        c_repeated = c.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        a_repeated = a.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        lamb_repeated = lamb.repeat_interleave(n_Monte_Carlo, dim=0) # shape (n*n_Monte_Carlo, A)\n",
    "        tfp_vec_repeated = tfp_vec.repeat_interleave(n_Monte_Carlo, dim=0)\n",
    "        delta_vec_repeated = delta_vec.repeat_interleave(n_Monte_Carlo, dim=0)\n",
    "        \n",
    "        # transitions of the exogenous processes\n",
    "        tfp_vec_next = torch.exp(params.rho_tfp*torch.log(tfp_vec_repeated) + e_tfp) # innovation_vec[:, -2]\n",
    "        delta_vec_next = delta_vec_repeated + e_delta #innovation_vec[:, -1]\n",
    "    \n",
    "        K_vec_next = torch.sum(h_matrix_next, 1)\n",
    "    \n",
    "        ## Exogeneous labour supply. \n",
    "        l_matrix_next = torch.zeros_like(h_matrix_next)\n",
    "        l_matrix_next[:, 0:params.A_tilde+1] = params.l_s #supply ls units of labor from 0 until A_tilde (included).\n",
    "        \n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec_next = torch.sum(l_matrix_next, 1)\n",
    "    \n",
    "        ## get factor prices (wages and interest rate)\n",
    "        r_next = interest_rate(K_vec_next, L_vec_next, delta_vec_next, tfp_vec_next, params)\n",
    "        w_next = wage(K_vec_next, L_vec_next, tfp_vec_next, params)\n",
    "    \n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth_next = h_matrix_next*r_next.view(-1,1) + l_matrix_next*w_next.view(-1,1)\n",
    "    \n",
    "        ## Consumption curent period\n",
    "        if debug == False:\n",
    "            PE_next = model_normalized(neural_net, tfp_vec_next, wealth_next, h_matrix_next, params)\n",
    "            ## Infer consumption\n",
    "            c_next_guess = c_guess_function(PE_next, wealth_next)\n",
    "            ## Infer capital decision\n",
    "            a_next_guess = wealth_next - c_next_guess\n",
    "            ## Truncation\n",
    "            if truncate_a == True:\n",
    "                a_next = torch.where(a_next_guess > 0, a_next_guess, 0.0)\n",
    "                c_next = torch.clamp(wealth_next - a_next, tol)\n",
    "            else:\n",
    "                a_next = a_next_guess\n",
    "                c_next = c_next_guess\n",
    "            ## Dummy value\n",
    "            lamb_next = torch.zeros_like(c_next)\n",
    "        else:\n",
    "            a_next = wealth_next*params.mult_wealth.view(1, -1)\n",
    "            c_next = wealth_next - a_next\n",
    "    \n",
    "        # Each column is the euler equation for one agent\n",
    "        # rows are observations\n",
    "        #s = c_next[:, 1:params.nb_agents].shape\n",
    "        #print(f\"shape c next: {s}\")\n",
    "        u_prime_next = params.u_prime(c_next)\n",
    "    \n",
    "        # Calculate beta (u'-1){E[beta*u'(c^{h+1}_{t+1})*r_{t+1} + lambda^{h}_{t}]}\n",
    "        vals = u_prime_next[:, 1:params.nb_agents]*r_next.view(-1,1)\n",
    "        \n",
    "        # Expectation\n",
    "        ## Keep age as column. Observations in rows.\n",
    "        Expectation = params.beta*torch.sparse.mm(D_sparse, vals)\n",
    "        # See for instance: https://julia.quantecon.org/dynamic_programming/ifp.html\n",
    "        ## When non-binding: usual Euler. Otherwise, savings = 0\n",
    "        max_two_values = torch.maximum(Expectation, params.u_prime(wealth[:, 0:params.nb_agents-1]))**(-1.0/params.gamma)\n",
    "    \n",
    "        # Euler equation error\n",
    "        ## rows: observations (state vector)\n",
    "        ## columns: age group\n",
    "        EEE = (max_two_values/ c[:, 0:params.nb_agents-1]) - 1\n",
    "        EEE = EEE.numpy()\n",
    "\n",
    "    if do_plots == True:\n",
    "        age_vec = np.arange(params.nb_agents-1) + 1\n",
    "        df_EEE_age = pd.DataFrame(np.abs(EEE))\n",
    "        line_styles = ['-', '--', ':', '-.']\n",
    "        colors = sns.color_palette(\"Set2\", 4)\n",
    "        fig, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        ax1.plot(age_vec, df_EEE_age.mean(), color = colors[0], linestyle = line_styles[0], label=\"Mean\")\n",
    "        ax1.plot(age_vec, df_EEE_age.quantile(0.1),color = colors[1], linestyle = line_styles[1], label=\"P10\")\n",
    "        ax1.plot(age_vec, df_EEE_age.quantile(0.9), color = colors[2], linestyle = line_styles[2], label=\"P90\")\n",
    "        ax1.plot(age_vec, df_EEE_age.max(), color = colors[3], linestyle = line_styles[3], label=\"Maximum\")\n",
    "        #ax1.plot(age_vec, df_EEE_age.min())\n",
    "        ax1.set_ylabel(\"Average EEE\")\n",
    "        ax1.set_xticks(range(1, params.nb_agents))\n",
    "        ax1.set_xlabel(\"Age\")\n",
    "        plt.tight_layout()\n",
    "        if name_fig != None:\n",
    "            plt.savefig( name_fig, bbox_inches=\"tight\", dpi=600)\n",
    "        plt.show()\n",
    "        \n",
    "    return EEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f49f720-95b8-4178-93d1-2d50a576fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_policy(neural_net, params, d_replacement, do_plots=False, name_fig=None, truncate_a=True, set_ticks = True, show_min = True, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Function to compare exact policy and approximation.\n",
    "    Exact policy is correct if using log-utility and if only first age group works (l_s = 1.0).\n",
    "    Otherwise, no analytic solution. Still display plots.\n",
    "    \"\"\"\n",
    "    nb_draws = 100 #each draw has params.M draws\n",
    "    with torch.no_grad():\n",
    "        # Draw nb_draws times M draws from ergodic distribution\n",
    "        state_vec = generate_n_batches(nb_draws, d_replacement)\n",
    "    \n",
    "        h_matrix = state_vec[:,:-2] #distribution of capital\n",
    "        # exo states\n",
    "        tfp_vec = state_vec[:, -2]\n",
    "        delta_vec = state_vec[:, -1]\n",
    "    \n",
    "        # infer sum of capital\n",
    "        K_vec = torch.sum(h_matrix, 1)\n",
    "    \n",
    "        ## Exogeneous labour supply.\n",
    "        l_matrix = torch.zeros_like(h_matrix)\n",
    "        l_matrix[:, 0:params.A_tilde+1] = params.l_s #supply ls units of labor from 0 until A_tilde (included).\n",
    "        \n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec = torch.sum(l_matrix, 1)\n",
    "    \n",
    "        ## get factor prices (wages and interest rate)\n",
    "        r = interest_rate(K_vec, L_vec, delta_vec, tfp_vec, params)\n",
    "        w = wage(K_vec, L_vec, tfp_vec, params)\n",
    "    \n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth = h_matrix*r.view(-1,1) + l_matrix*w.view(-1,1)\n",
    "    \n",
    "        ## Consumption curent period\n",
    "        PE_t = model_normalized(neural_net, tfp_vec, wealth, h_matrix, params)\n",
    "        ## Infer consumption\n",
    "        c_guess = c_guess_function(PE_t, wealth)\n",
    "        ## Infer capital decision\n",
    "        a_guess = wealth - c_guess\n",
    "        ## Truncation\n",
    "        if truncate_a == True:\n",
    "            a_vec = torch.where(a_guess > 0, a_guess, 0.0)\n",
    "            c_vec = torch.clamp(wealth - a_vec, min=tol)\n",
    "        else:\n",
    "            a_vec = a_guess\n",
    "            c_vec = c_guess\n",
    "        ## Dummy variable when budget constraint is binding\n",
    "        lamb_vec = torch.zeros_like(c_guess)\n",
    "        lamb_vec[:, 0:params.nb_agents-1] = torch.where(a_vec[:,0:params.nb_agents-1] == 0, 1, 0.0)\n",
    "        \n",
    "    a_vec = a_vec.numpy() \n",
    "    c_vec = c_vec.numpy() \n",
    "    lamb_vec = lamb_vec.numpy()\n",
    "    wealth_vec = wealth.numpy()\n",
    "    \n",
    "    # Theoretical value when:\n",
    "    # * l = 1 for first generation only\n",
    "    # * no borrowing constraint\n",
    "    with torch.no_grad():\n",
    "        theory_a = wealth*params.mult_wealth.view(1, -1)\n",
    "        theory_c = wealth - theory_a\n",
    "    \n",
    "    a_vec_theory = theory_a.numpy()\n",
    "    c_vec_theory = theory_c.numpy()\n",
    "    \n",
    "    c_mean = np.mean(c_vec, 0)\n",
    "    c_p10 = np.quantile(c_vec, 0.1, 0)\n",
    "    c_p25 = np.quantile(c_vec, 0.25, 0)\n",
    "    c_p75 = np.quantile(c_vec, 0.75, 0)\n",
    "    c_p90 = np.quantile(c_vec, 0.9, 0)\n",
    "    c_min = np.min(c_vec, 0)\n",
    "\n",
    "    lamb_mean = np.mean(lamb_vec, 0)\n",
    "    lamb_p10 = np.quantile(lamb_vec, 0.1, 0)\n",
    "    lamb_p25 = np.quantile(lamb_vec, 0.25, 0)\n",
    "    lamb_p75 = np.quantile(lamb_vec, 0.75, 0)\n",
    "    lamb_p90 = np.quantile(lamb_vec, 0.9, 0)\n",
    "    lamb_min = np.min(lamb_vec, 0)\n",
    "\n",
    "    a_mean = np.mean(a_vec, 0)\n",
    "    a_p10 = np.quantile(a_vec, 0.1, 0)\n",
    "    a_p25 = np.quantile(a_vec, 0.25, 0)\n",
    "    a_p75 = np.quantile(a_vec, 0.75, 0)\n",
    "    a_p90 = np.quantile(a_vec, 0.9, 0)\n",
    "    a_min = np.min(a_vec, 0)\n",
    "\n",
    "    #wealth \n",
    "    wealth_mean = np.mean(wealth_vec, 0)\n",
    "    wealth_p10 = np.quantile(wealth_vec, 0.1, 0)\n",
    "    wealth_p25 = np.quantile(wealth_vec, 0.25, 0)\n",
    "    wealth_p75 = np.quantile(wealth_vec, 0.75, 0)\n",
    "    wealth_p90 = np.quantile(wealth_vec, 0.9, 0)\n",
    "    wealth_min = np.min(wealth_vec, 0)\n",
    "    \n",
    "    c_mean_theory = np.mean(c_vec_theory , 0)\n",
    "    c_p10_theory = np.quantile(c_vec_theory , 0.1, 0)\n",
    "    c_p25_theory = np.quantile(c_vec_theory , 0.25, 0)\n",
    "    c_p75_theory = np.quantile(c_vec_theory , 0.75, 0)\n",
    "    c_p90_theory = np.quantile(c_vec_theory , 0.9, 0)\n",
    "    \n",
    "    a_mean_theory = np.mean(a_vec_theory , 0)\n",
    "    a_p10_theory = np.quantile(a_vec_theory , 0.1, 0)\n",
    "    a_p25_theory = np.quantile(a_vec_theory , 0.25, 0)\n",
    "    a_p75_theory = np.quantile(a_vec_theory , 0.75, 0)\n",
    "    a_p90_theory = np.quantile(a_vec_theory , 0.9, 0)\n",
    "    \n",
    "    age_vec = np.arange(params.nb_agents) + 1\n",
    "    if show_min == False:\n",
    "        line_styles = ['-', '--', ':', '-.']\n",
    "        colors = sns.color_palette(\"Set2\", 3)\n",
    "    else:\n",
    "        line_styles = ['-', '--', ':', '-.', 'dashdot']\n",
    "        colors = sns.color_palette(\"Set2\", 4)\n",
    "        \n",
    "    if do_plots==True:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        #ax1.scatter(age_vec, a_mean)\n",
    "        ax1.plot(age_vec, a_mean, label=\"Mean\", color = colors[0], linestyle = line_styles[0])\n",
    "        ax1.plot(age_vec, a_p10, label=\"P10\", color = colors[1], linestyle = line_styles[1])\n",
    "        ax1.plot(age_vec, a_p90, label=\"P90\", color = colors[2], linestyle = line_styles[2])\n",
    "        #ax1.plot(age_vec, a_mean_theory, label=\"mean analytic (l_0 = 1)\")\n",
    "        ax1.axvline(x = params.A_tilde+1, color = 'black', linestyle = line_styles[3], alpha=0.5)\n",
    "        if show_min == True:\n",
    "            ax1.plot(age_vec, a_min, label=\"Minimum\", color = colors[3], linestyle = line_styles[4])\n",
    "        ax1.set_ylabel(\"Capital\")\n",
    "        ax1.set_xlabel(\"Age\")\n",
    "        if set_ticks == True:\n",
    "            ax1.set_xticks(range(1, params.nb_agents+1))\n",
    "        ax1.legend()\n",
    "        \n",
    "        #ax2.scatter(age_vec, c_mean, label=\"mean consumption\")\n",
    "        ax2.plot(age_vec, c_mean, label=\"mean\", color = colors[0],linestyle = line_styles[0])\n",
    "        ax2.plot(age_vec, c_p10, label=\"P10\", color = colors[1], linestyle = line_styles[1])\n",
    "        ax2.plot(age_vec, c_p90, label=\"P90\", color = colors[2], linestyle = line_styles[2])\n",
    "        #ax2.plot(age_vec, c_mean_theory, label=\"mean analytic (l_0 = 1)\")\n",
    "        ax2.axvline(x = params.A_tilde+1, color = 'black', linestyle = '-.', alpha=0.5)\n",
    "        if show_min == True:\n",
    "            ax2.plot(age_vec, c_min, label=\"Minimum\", color = colors[3], linestyle = line_styles[4])\n",
    "        ax2.set_ylabel(\"Consumption\")\n",
    "        ax2.set_xlabel(\"Age\")\n",
    "        if set_ticks == True:\n",
    "            ax2.set_xticks(range(1, params.nb_agents+1))\n",
    "        #ax2.legend()\n",
    "    \n",
    "        ax3.plot(age_vec, lamb_mean, label=\"mean\", color = colors[0], linestyle = line_styles[0])\n",
    "        ax3.plot(age_vec, lamb_p10, label=\"P10\", color = colors[1], linestyle = line_styles[1])\n",
    "        ax3.plot(age_vec, lamb_p90, label=\"P90\", color = colors[2], linestyle = line_styles[2])\n",
    "        ax3.axvline(x = params.A_tilde+1, color = 'black', linestyle = '-.', alpha=0.5)\n",
    "        if show_min == True:\n",
    "            ax3.plot(age_vec, lamb_min, label=\"Minimum\", color = colors[3], linestyle = line_styles[4])\n",
    "        ax3.set_ylabel(\"Lagrange mult. indicator\")\n",
    "        ax3.set_xlabel(\"Age\")\n",
    "        if set_ticks == True:\n",
    "            ax3.set_xticks(range(1, params.nb_agents+1))\n",
    "        #ax3.legend()\n",
    "    \n",
    "        ax4.plot(age_vec, wealth_mean, label=\"mean\", color = colors[0], linestyle = line_styles[0])\n",
    "        ax4.plot(age_vec, wealth_p10, label=\"P10\", color = colors[1], linestyle = line_styles[1])\n",
    "        ax4.plot(age_vec, wealth_p90, label=\"P90\", color = colors[2], linestyle = line_styles[2])\n",
    "        ax4.axvline(x = params.A_tilde+1, color = 'black', linestyle = '-.', alpha=0.5)\n",
    "        if show_min == True:\n",
    "            ax4.plot(age_vec, wealth_min, label=\"Minimum\", color = colors[3], linestyle = line_styles[4])\n",
    "        ax4.set_ylabel(\"Wealth\")\n",
    "        ax4.set_xlabel(\"Age\")\n",
    "        if set_ticks == True:\n",
    "            ax4.set_xticks(range(1, params.nb_agents+1))\n",
    "        #ax4.legend()\n",
    "        plt.tight_layout()\n",
    "        if name_fig != None:\n",
    "            plt.savefig( name_fig, bbox_inches=\"tight\", dpi=600)\n",
    "        plt.show()\n",
    "        \n",
    "    # Euler equation error\n",
    "    nb_draws_state_vec = 1000 ## Draws state vector\n",
    "    EEE_MC = evaluate_accuracy_pytorch_MC(neural_net, nb_draws_state_vec, 100, params, d_replacement, debug = False, truncate_a = truncate_a)\n",
    "    mean_EEE_MC = np.mean(np.abs(EEE_MC))\n",
    "    P50_EEE_MC = np.median(np.abs(EEE_MC))\n",
    "    max_EEE_MC = np.max(np.abs(EEE_MC))\n",
    "    print(f\"------------------------------------------------------------\")\n",
    "    print(f\"------------------------------------------------------------\")\n",
    "    print(f\"Mean Euler equation error (MC): {mean_EEE_MC}\")\n",
    "    print(f\"Median Euler equation error (MC): {P50_EEE_MC}\")\n",
    "    print(f\"Max Euler equation error (MC): {max_EEE_MC}\")\n",
    "    print(f\"------------------------------------------------------------\")\n",
    "    EEE_Gaussian = evaluate_accuracy_pytorch_Gaussian(neural_net, nb_draws_state_vec, params, d_replacement, debug = False, truncate_a = truncate_a)\n",
    "    mean_EEE_Gaussian = np.mean(np.abs(EEE_Gaussian))\n",
    "    P50_EEE_Gaussian = np.median(np.abs(EEE_Gaussian))\n",
    "    max_EEE_Gaussian = np.max(np.abs(EEE_Gaussian))\n",
    "    print(f\"Mean Euler equation error (Gaussian): {mean_EEE_Gaussian}\")\n",
    "    print(f\"Median Euler equation error (Gaussian): {P50_EEE_Gaussian}\")\n",
    "    print(f\"Max Euler equation error (Gaussian): {max_EEE_Gaussian}\")\n",
    "    print(f\"------------------------------------------------------------\")\n",
    "    print(f\"------------------------------------------------------------\")\n",
    "    return mean_EEE_Gaussian, P50_EEE_Gaussian, max_EEE_Gaussian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad693454-dda9-4a15-8c77-922d9464e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_functions(neural_net, params, x_variable = \"Wealth\", decision_name=\"consumption\", \n",
    "                            plot_share = False, plot_deviation_from_SS = False, nb_points = 200, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Function to plot decision functions.\n",
    "    Move total wealth for one age group, while keeping the rest unchanged.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():            \n",
    "        consumption_age = torch.ones(params.nb_agents, nb_points)\n",
    "        \n",
    "        delta_vec = params.mean_delta*torch.ones(nb_points)\n",
    "        tfp_vec = torch.ones(nb_points)\n",
    "        h_matrix = torch.zeros((nb_points, params.nb_agents))\n",
    "        h_matrix[:,1:] = assets_SS #initial_state_vector #first periods\n",
    "        \n",
    "        K_vec = torch.sum(h_matrix, 1)\n",
    "        \n",
    "        ## Exogeneous labour supply.\n",
    "        l_matrix = torch.zeros_like(h_matrix)\n",
    "        l_matrix[:, 0:params.A_tilde+1] = params.l_s #supply ls units of labor from 0 until A_tilde (included).\n",
    "        \n",
    "        ## Total labour supply (useless calculations, but then we can generalize the code)\n",
    "        L_vec = torch.sum(l_matrix, 1)\n",
    "        \n",
    "        r = interest_rate(K_vec, L_vec, delta_vec, tfp_vec, params)\n",
    "        w = wage(K_vec, L_vec, tfp_vec, params)\n",
    "        ## calculate wealth, before consumption decision made\n",
    "        wealth = h_matrix*r.view(-1,1) + l_matrix*w.view(-1,1)\n",
    "\n",
    "        # Prepare grids for plotting\n",
    "        if x_variable == \"Wealth\":\n",
    "            wealth_grid = torch.linspace(0.25, 2.0, nb_points)\n",
    "            x_grid = wealth_grid\n",
    "        elif x_variable == \"Tfp\":\n",
    "            # log(z_t) has mean 0 and variance std_tfp^2/(1 - rho^2)\n",
    "            std_dev_log = (params.std_tfp**2)/(1 - params.rho_tfp**2)\n",
    "            mult_std_dev = 100\n",
    "            log_tfp_grid = torch.linspace(0 - mult_std_dev*std_dev_log, 0 + mult_std_dev*std_dev_log, nb_points)\n",
    "            tfp_vec = torch.exp(log_tfp_grid)\n",
    "            x_grid = tfp_vec\n",
    "        else:\n",
    "            raise Exception(f'x_variable: {x_variable}')\n",
    "            \n",
    "        # Set up the color map\n",
    "        cmap = cm.viridis\n",
    "        norm = mcolors.Normalize(vmin=1, vmax=params.nb_agents)\n",
    "    \n",
    "        fig, ax = plt.subplots()  # Create a figure and axis\n",
    "    \n",
    "        for age_group in range(0, params.nb_agents):\n",
    "            # Map age_group to a color from the colormap\n",
    "            color = cmap(norm(age_group))\n",
    "        \n",
    "            wealth_age = torch.clone(wealth)\n",
    "            if x_variable == \"Wealth\":\n",
    "                wealth_age[:, age_group] = wealth_grid\n",
    "                \n",
    "            PE_t = model_normalized(neural_net, tfp_vec, wealth_age, h_matrix, params)\n",
    "            c_guess = c_guess_function(PE_t, wealth_age)\n",
    "            a_guess = wealth_age - c_guess\n",
    "            # Truncated\n",
    "            a = torch.where(a_guess > 0, a_guess, 0.0)\n",
    "            c = torch.clamp(wealth_age - a, min=tol)\n",
    "            ## Dummy variable when budget constraint is binding\n",
    "            lamb = torch.zeros_like(c)\n",
    "            lamb[:, 0:params.nb_agents-1] = torch.where(a[:,0:params.nb_agents-1] == 0, 1, 0.0)\n",
    "        \n",
    "            if decision_name ==\"consumption\":\n",
    "                to_plot = c[:, age_group]\n",
    "                label = \"Consumption\"\n",
    "            elif decision_name ==\"savings\":\n",
    "                to_plot = a[:, age_group]\n",
    "                label = \"Savings\"\n",
    "            elif decision_name ==\"lambda\":\n",
    "                to_plot = lamb[:, age_group]\n",
    "                label = \"Lagange Mult. Ind.\"\n",
    "            elif decision_name ==\"PE\":\n",
    "                # Add last generation\n",
    "                if age_group == params.nb_agents-1:\n",
    "                    to_plot = params.u_prime(wealth_age[:,-1])/params.u_prime(wealth_age[int(nb_points/2),-1])\n",
    "                else:\n",
    "                    to_plot = PE_t[:, age_group]\n",
    "                label = \"Cond. Expectation\"\n",
    "            else: \n",
    "                raise Exception(f'decision_name: {decision_name}')\n",
    "            if plot_share == True:\n",
    "                to_plot = to_plot/wealth_age[:, age_group]\n",
    "                label = label + \" Share\"\n",
    "            if plot_deviation_from_SS == True:\n",
    "                SS_val = to_plot[int(nb_points/2)]\n",
    "                if decision_name !=\"lambda\":\n",
    "                    to_plot = 100*((to_plot-SS_val)/SS_val)\n",
    "                    label = label + \" (% Dev. from SS)\"\n",
    "            ax.plot(x_grid, to_plot.numpy(), color=color)\n",
    "        if (plot_share == False) & (decision_name != \"lambda\") :\n",
    "            ax.plot(x_grid, x_grid, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "        ax.set_ylabel(label)\n",
    "        ax.set_xlabel(x_variable)\n",
    "        \n",
    "        # Create colorbar\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        sm.set_array([])  # Only needed for older versions of matplotlib\n",
    "        \n",
    "        # Set ticks to integers\n",
    "        cbar = fig.colorbar(sm, ax=ax, ticks=range(1, params.nb_agents+1))  # Specify 'ax=ax' to resolve the warning\n",
    "        cbar.set_label('Age Group')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743fc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_norm(parameters, norm_type=2.0):\n",
    "    \"\"\"\n",
    "    Compute norm over gradients of model parameters.\n",
    "\n",
    "    :param parameters:\n",
    "        the model parameters for gradient norm calculation. Iterable of\n",
    "        Tensors or single Tensor\n",
    "    :param norm_type:\n",
    "        type of p-norm to use\n",
    "\n",
    "    :returns:\n",
    "        the computed gradient norm\n",
    "    \"\"\"\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p is not None and p.grad is not None]\n",
    "    total_norm = 0\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm.item() ** norm_type\n",
    "    return total_norm ** (1.0 / norm_type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fe858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, optimizer_name, lr, momentum, use_Lookahead, beta1=0.9, beta2=0.999):\n",
    "    \"\"\"\n",
    "    Function to create an optimizer. \n",
    "    Default values for betas = (0.9, 0.999)\n",
    "    \"\"\"\n",
    "    if optimizer_name == \"Adam\":\n",
    "        #Default vals\n",
    "        #torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1, beta2)) \n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "    elif optimizer_name == \"SGD-momentum\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr, momentum)\n",
    "    elif optimizer_name == \"Adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters()) #, lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr, weight_decay=1e-4)\n",
    "    elif optimizer_name == \"NAdam\":\n",
    "        # From from torch_optimizer import Nadam\n",
    "        # Nadam combines the benefits of ADAM and Nesterov momentum, making it slightly more aggressive in its updates.\n",
    "        # It can improve convergence speed and performance in some cases.\n",
    "        optimizer = torch.optim.NAdam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"MADGRAD\":\n",
    "        optimizer = MADGRAD(model.parameters(),lr=lr)\n",
    "    elif optimizer_name == \"AdamP\":\n",
    "        optimizer = AdamP(model.parameters(), lr=lr) \n",
    "    #elif optimizer_name == \"LBFGS\":\n",
    "    #    torch.optim.LBFGS(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise NameError(f\"optimizer {optimizer_name} unknown\")\n",
    "    if use_Lookahead == True:\n",
    "        optimizer = Lookahead(optimizer)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd0f49",
   "metadata": {},
   "source": [
    "## Variance estimation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2079d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a model, calculate the current variance of the loss\n",
    "#Brute force\n",
    "def calculate_variance_loss_model(model, params, nb_draws_loss):\n",
    "    model.eval() #eval mode\n",
    "    with torch.no_grad():        \n",
    "        Xms = torch.zeros(nb_draws_loss)\n",
    "        # Loop over realizations of loss function\n",
    "        for (j_index, j) in enumerate(range(0, nb_draws_loss)):\n",
    "            Xms[j] = Ξ_torch_MC(model, params)\n",
    "        # Calculate mean and variance:\n",
    "        var_loss = torch.var(Xms)\n",
    "        mean_loss = torch.mean(Xms)\n",
    "    model.train() #train mode\n",
    "    return var_loss, mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e84dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_gaussian(params, neural_net, nb_draws, d_replacement, grid_M, grid_N, debug=False, tol = torch.tensor([1e-6])):\n",
    "    \"\"\"\n",
    "    Calculate variance of the loss when joint gaussian assumption holds\n",
    "    Use var(f(s_m,e^i_m))\n",
    "    and cov(f(s_m,e^i_m), f(s_m,e^j_m))\n",
    "    Return: variance loss function on grid, var(resid), cov(resid)\n",
    "    \"\"\"\n",
    "    grid_T = torch.tensor(grid_M*grid_N/2)\n",
    "    grid_N = torch.tensor(grid_N)\n",
    "    # Calculate nb of batches required to get the right number of draws\n",
    "    if nb_draws < params.M:\n",
    "        nb_batches_var = params.M\n",
    "    else:\n",
    "        nb_batches_var = int(nb_draws/params.M)\n",
    "\n",
    "    # Calculate variance and covariance\n",
    "    with torch.no_grad(): \n",
    "        ## state vector\n",
    "        state_vec = generate_n_batches(nb_batches_var, d_replacement)\n",
    "        # endo state\n",
    "        #h_matrix = state_vec[:,:-2] #distribution of capital\n",
    "        # exo states\n",
    "        #tfp_vec = state_vec[:, -2]\n",
    "        #delta_vec = state_vec[:, -1]\n",
    "    \n",
    "        ## Get two independent innovation vectors\n",
    "        e_tfp_1, e_delta_1  = simulate_innovations(params, nb_draws)\n",
    "        innovation_vec_1 =  torch.column_stack((e_tfp_1, e_delta_1))\n",
    "        \n",
    "        e_tfp_2, e_delta_2   = simulate_innovations(params, nb_draws)\n",
    "        innovation_vec_2 =  torch.column_stack((e_tfp_2, e_delta_2))\n",
    "        \n",
    "        # residuals for n random grid points under 2 realizations of shocks        \n",
    "        R1 = Residuals_torch(neural_net, state_vec, innovation_vec_1, params, debug, tol)\n",
    "        R2 = Residuals_torch(neural_net, state_vec, innovation_vec_2, params, debug, tol)\n",
    "\n",
    "        ## Age-specific losses\n",
    "        # Reshape to (MN, nb_agents)\n",
    "        #R1_matrix = R1.view(params.nb_agents-1, params.MN).t()\n",
    "        #R2_matrix = R2.view(params.nb_agents-1, params.MN).t()\n",
    "        #R1_mean = torch.mean(R1_matrix, axis=0) #mean by age group\n",
    "        #R2_mean = torch.mean(R2_matrix, axis=0) #mean by age group\n",
    "        # Construct combinations\n",
    "        # mean\n",
    "        ##mean_val = 0.5*torch.mean(R1_mean) + 0.5*torch.mean(R2_mean)\n",
    "        ## Var\n",
    "        #var_R1 = torch.var(R1_matrix, axis=0) #var by age group\n",
    "        #var_R2 = torch.var(R2_matrix, axis=0) #var by age group\n",
    "        #var_val = 0.5*var_R1 + 0.5*var_R2\n",
    "\n",
    "        # Construct combinations\n",
    "        # mean\n",
    "        mean_val = 0.5*torch.mean(R1) + 0.5*torch.mean(R2)\n",
    "        \n",
    "        ## Var\n",
    "        var_R1 = torch.var(R1)\n",
    "        var_R2 = torch.var(R2)\n",
    "        var_val = 0.5*var_R1 + 0.5*var_R2\n",
    "        \n",
    "        ## Cov\n",
    "        cov_val = torch.cov(torch.column_stack((R1, R2)).T)[0,1]\n",
    "        \n",
    "        var_L = (1/(grid_T*(grid_N - 1)))*((grid_N**2 - 3*grid_N + 3)*(cov_val**2) + (2*(grid_N - 2)*cov_val + var_val)*var_val + 2*(grid_N - 1)*(var_val + (grid_N - 1)*cov_val)*(mean_val**2))\n",
    "        \n",
    "        return var_L, var_val, cov_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1d441",
   "metadata": {},
   "source": [
    "## Logging and other utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_flat(a):\n",
    "    \"\"\"\n",
    "    Function to flatten a list\n",
    "    \"\"\"\n",
    "    return list(np.array(a).flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ff7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSystemInfo():\n",
    "    \"\"\"\n",
    "    Get info on computer hardware\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info={}\n",
    "        info['platform']=platform.system()\n",
    "        info['platform-release']=platform.release()\n",
    "        info['platform-version']=platform.version()\n",
    "        info['architecture']=platform.machine()\n",
    "        info['hostname']=socket.gethostname()\n",
    "        info['ip-address']=socket.gethostbyname(socket.gethostname())\n",
    "        info['mac-address']=':'.join(re.findall('..', '%012x' % uuid.getnode()))\n",
    "        info['processor']=platform.processor()\n",
    "        info['ram']=str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"\n",
    "        return json.dumps(info)\n",
    "    except Exception as e:\n",
    "        logging.exception(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
